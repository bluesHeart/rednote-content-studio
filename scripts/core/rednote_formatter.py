#!/usr/bin/env python3
"""
å°çº¢ä¹¦æ’ç‰ˆæ ¼å¼åŒ–å™¨

å°†å†…å®¹æ ¼å¼åŒ–ä¸ºå°çº¢ä¹¦é£æ ¼ï¼ŒåŒ…æ‹¬ï¼š
- ä½¿ç”¨ç›²æ–‡ç©ºæ ¼ä¿æŒç©ºè¡Œ
- æ™ºèƒ½æ’å…¥ emoji
- åº”ç”¨åˆ†éš”çº¿å’Œè£…é¥°
"""

from __future__ import annotations

import json
import logging
import re
from dataclasses import dataclass
from typing import Optional

from .markdown_parser import ContentBlock, BlockType
from .content_splitter import PageContent
from .markdown_text_normalizer import MarkdownTextNormalizer

try:
    from ..constants.rednote_chars import (
        BRAILLE_BLANK,
        PARAGRAPH_SEPARATOR,
        make_blank_lines,
        make_numbered_item,
        make_emphasis,
        make_divider,
        make_title,
        NUMBER_EMOJIS,
        LIST_MARKERS,
        QUOTE_MARKS,
    )
    from ..constants.emoji_library import (
        get_emotion_emoji,
        get_scene_emoji,
        get_topic_emoji,
        get_indicator,
        INDICATOR_EMOJIS,
    )
except ImportError:
    from constants.rednote_chars import (
        BRAILLE_BLANK,
        PARAGRAPH_SEPARATOR,
        make_blank_lines,
        make_numbered_item,
        make_emphasis,
        make_divider,
        make_title,
        NUMBER_EMOJIS,
        LIST_MARKERS,
        QUOTE_MARKS,
    )
    from constants.emoji_library import (
        get_emotion_emoji,
        get_scene_emoji,
        get_topic_emoji,
        get_indicator,
        INDICATOR_EMOJIS,
    )

logger = logging.getLogger(__name__)


@dataclass
class FormattedPage:
    """æ ¼å¼åŒ–åçš„é¡µé¢"""
    page_number: int
    content: str
    char_count: int
    emoji_count: int
    has_proper_spacing: bool
    image_urls: list[str] = None  # è¯¥é¡µå…³è”çš„å›¾ç‰‡ URL / è·¯å¾„
    image_slots: list[int] = None  # å›¾ç‰‡åœ¨æ–‡æœ¬å—ä¸­çš„æ’å…¥é”šç‚¹ï¼ˆå—é—´ä½ç½®ï¼‰

    def __post_init__(self):
        if self.image_urls is None:
            self.image_urls = []
        if self.image_slots is None:
            self.image_slots = []


class RedNoteFormatter:
    """å°çº¢ä¹¦æ’ç‰ˆæ ¼å¼åŒ–å™¨"""

    FORMAT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ª 25 å²çš„å°çº¢ä¹¦åšä¸»ï¼ŒæŠŠå†…å®¹æ”¹å†™æˆä½ è‡ªå·±å‘å¸–çš„é£æ ¼ã€‚

ä½ çš„é£æ ¼ç‰¹ç‚¹ï¼š
- è¯´äººè¯ï¼åƒè·Ÿæœ‹å‹èŠå¤©ä¸€æ ·ï¼Œåˆ«ç«¯ç€
- å¥å­è¦çŸ­ï¼Œä¸€å¥è¯åˆ«è¶…è¿‡ 20 å­—
- å¤šæ¢è¡Œï¼Œçœ‹ç€ä¸ç´¯
- emoji è¦è‡ªç„¶ï¼Œåˆ«ç¡¬å¡ï¼Œç”¨å°±ç”¨å¹´è½»äººçˆ±ç”¨çš„ï¼ˆâœ¨ğŸ« ğŸ’€ğŸ˜­ğŸ¤¯ğŸ”¥ğŸ’¡ğŸ‘€ğŸ¥¹ç­‰ï¼‰
- ç»å¯¹ä¸è¦ç”¨ã€ã€‘è¿™ç§è€åœŸæ ‡æ³¨
- ç»å¯¹ä¸è¦ç”¨ 1ï¸âƒ£2ï¸âƒ£3ï¸âƒ£ è¿™ç§æ•°å­—emojiï¼ˆå¤ªä¸‘äº†ï¼‰
- åˆ—è¡¨å°±ç”¨ Â· æˆ–è€… - æˆ–è€…ç›´æ¥æ¢è¡Œ
- ä¸è¦å†™"è®°å¾—ç‚¹èµæ”¶è—"ä¹‹ç±»çš„ï¼ˆå¤ªæ²¹è…»ï¼‰
- ä¸è¦é—®"æƒ³è®©æˆ‘..."è¿™ç§AIè…”
- å¦‚æœç¢°åˆ°è¶…é•¿ä»£ç å—ï¼ˆ> 15è¡Œï¼‰ï¼Œåªä¿ç•™æœ€å…³é”®çš„ 5-8 è¡Œï¼ŒåŠ ä¸€å¥"å®Œæ•´ä»£ç å¤ªé•¿äº†æ”¾è¯„è®ºåŒº"
- æ¯é¡µæ€»å­—æ•°æ§åˆ¶åœ¨ 400 å­—ä»¥å†…ï¼è¶…äº†å°±ç²¾ç®€

ç©ºè¡Œç”¨ â € (U+2800 ç›²æ–‡ç©ºæ ¼)ï¼Œåˆ«ç”¨æ™®é€šç©ºè¡Œã€‚

è¿”å›JSONï¼š
{
    "title": "æ”¹å†™åçš„æ ‡é¢˜ï¼ˆè¦å¸å¼•äººä½†åˆ«æ ‡é¢˜å…šï¼‰",
    "sections": [{"content": "æ”¹å†™åçš„æ­£æ–‡"}],
    "ending": "ç®€çŸ­æ”¶å°¾ï¼Œå¯ä»¥ä¸ºç©º"
}"""

    DOCUMENT_CONTINUITY_SYSTEM_PROMPT = """ä½ æ˜¯åŒä¸€ä½å°çº¢ä¹¦ä½œè€…çš„â€œæ•´ç¯‡æ”¹ç¨¿ç¼–è¾‘â€ã€‚
ä½ çš„ä»»åŠ¡ä¸æ˜¯é€é¡µæ¶¦è‰²ï¼Œè€Œæ˜¯æŠŠæ•´ç¯‡åˆ†é¡µè‰ç¨¿é‡å†™æˆâ€œä¸€æ¡è¿ç»­å™äº‹â€çš„å¤šé¡µå›¾æ–‡ã€‚

æ ¸å¿ƒè¦æ±‚ï¼ˆå¿…é¡»åŒæ—¶æ»¡è¶³ï¼‰ï¼š
1) é¡µæ•°é”å®šï¼šè¾“å‡º pages æ•°é‡å¿…é¡»ä¸è¾“å…¥å®Œå…¨ä¸€è‡´ã€‚
2) è¿ç»­å™äº‹ï¼š
   - ç¬¬1é¡µå¯ä»¥æœ‰çŸ­æ ‡é¢˜/é’©å­ã€‚
   - ç¬¬2é¡µåŠä»¥åå¿…é¡»æ‰¿æ¥ä¸Šä¸€é¡µï¼Œä¸å…è®¸æ¯é¡µéƒ½åƒæ–°å¼€ä¸€æ¡å¸–å­ã€‚
   - ç¦æ­¢â€œé‡æ–°èµ·é¢˜ã€é‡å¤æ€»ç»“ã€é‡å¤å¼€åœºç™½â€ã€‚
3) å°çº¢ä¹¦èŠ‚å¥ï¼š
   - çŸ­å¥ä¼˜å…ˆï¼Œå°½é‡ä¸€è¡Œä¸€å¥æˆ–ä¸¤å¥ã€‚
   - æ¯é¡µç”¨ 3-8 ä¸ªè‡ªç„¶æ¢è¡Œç»„ç»‡ä¿¡æ¯ï¼Œé¿å…æ•´é¡µå•æ®µé•¿æ–‡ã€‚
   - æ®µè½è¦è½»ï¼Œä¸å†™å¤§æ®µè¯´æ˜ä¹¦å¼é•¿æ–‡ã€‚
   - éæ­¥éª¤é¡µä¸è¦æ•´é¡µéƒ½å†™æˆé¡¹ç›®ç¬¦å·åˆ—è¡¨ï¼ˆé¿å…è¯´æ˜ä¹¦æ„Ÿï¼‰ã€‚
   - è¯­æ°”è‡ªç„¶å£è¯­åŒ–ï¼Œä½†ä¸è¦æ²¹è…»å£æ’­è…”ã€‚
4) ä¿¡æ¯å®Œæ•´ï¼šå·¥å…·åã€æ­¥éª¤é¡ºåºã€å…³é”®æœ¯è¯­ã€å…³é”®ä»£ç ç‰‡æ®µä¸èƒ½ä¸¢ã€‚
5) å›¾ç‰‡æ„ŸçŸ¥ï¼šè‹¥æŸé¡µæ ‡æ³¨æœ‰å›¾ç‰‡ï¼Œä¸è¦æŠŠè¯¥é¡µå†™æˆçº¯æŠ½è±¡æ€»ç»“ï¼Œåº”ä¸è¯¥æ­¥éª¤/ä¿¡æ¯åŒæ­¥ã€‚
   - è‹¥è¾“å…¥æ­£æ–‡é‡Œå‡ºç° <IMG_1>ã€<IMG_2>... å›¾ç‰‡é”šç‚¹ï¼Œå¿…é¡»åŸæ ·ä¿ç•™ã€‚
   - æ¯ä¸ªé”šç‚¹å¿…é¡»ä¸”åªèƒ½å‡ºç°ä¸€æ¬¡ï¼Œé¡ºåºä¸å¯æ”¹å˜ã€‚
   - é”šç‚¹åº”ä½œä¸ºç‹¬ç«‹æ®µè½ï¼ˆå•ç‹¬ä¸€è¡Œï¼‰æ”¾ç½®åœ¨æœ€åˆé€‚çš„ä½ç½®ã€‚
6) æ¸…ç†è¯­æ³•å™ªå£°ï¼šå»æ‰ Markdown ç—•è¿¹ï¼ˆå¦‚ **ã€`ã€```ã€[]()ï¼‰ã€‚
7) ç¯‡å¹…æ§åˆ¶ï¼šæ¯é¡µå»ºè®® 180-360 å­—ï¼Œå…è®¸å°‘é‡æµ®åŠ¨ï¼›ç¡¬ä¸Šé™ 430 å­—ï¼Œè¶…å‡ºå¿…é¡»ä¸»åŠ¨å‹ç¼©ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼ï¼‰ï¼š
åªè¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæ–‡å­—ã€‚
{
  "pages": [
    {"content": "ç¬¬1é¡µå†…å®¹"},
    {"content": "ç¬¬2é¡µå†…å®¹"}
  ]
}
"""

    BLOCK_SEPARATOR = PARAGRAPH_SEPARATOR
    IMAGE_TOKEN_RE = re.compile(r"<IMG_(\d+)>")

    def __init__(self, llm_client=None, tone_system_prompt: str | None = None):
        """
        åˆå§‹åŒ–æ ¼å¼åŒ–å™¨

        Args:
            llm_client: å¯é€‰çš„ LLMClient å®ä¾‹
            tone_system_prompt: å¯é€‰çš„è‡ªå®šä¹‰è¯­æ°” system promptï¼Œæ›¿æ¢é»˜è®¤çš„ FORMAT_SYSTEM_PROMPT
        """
        self.llm_client = llm_client
        self._tone_system_prompt = tone_system_prompt
        self._normalizer = MarkdownTextNormalizer()

    def _format_heading(self, block: ContentBlock, is_title: bool = False) -> str:
        """æ ¼å¼åŒ–æ ‡é¢˜"""
        text = self._normalizer.normalize_line(block.content)
        return text

    def _format_paragraph(self, block: ContentBlock, context: str = '') -> str:
        """æ ¼å¼åŒ–æ®µè½"""
        return self._normalizer.normalize_multiline(block.content)

    def _format_list(self, block: ContentBlock) -> str:
        """æ ¼å¼åŒ–åˆ—è¡¨"""
        lines = []
        for item in block.items:
            normalized = self._normalizer.normalize_line(item)
            if normalized.startswith("Â· "):
                lines.append(normalized)
            else:
                lines.append(f"Â· {normalized}")
        return '\n'.join(lines)

    def _format_quote(self, block: ContentBlock) -> str:
        """æ ¼å¼åŒ–å¼•ç”¨"""
        lines = self._normalizer.normalize_multiline(block.content).split('\n')
        formatted_lines = []

        for line in lines:
            if not line.strip():
                continue
            formatted_lines.append(f"{QUOTE_MARKS['line']} {line}")

        return '\n'.join(formatted_lines)

    def _format_code(self, block: ContentBlock) -> str:
        """æ ¼å¼åŒ–ä»£ç å—"""
        return self._normalizer.compact_code_block(block.content, block.language)

    def _format_block(self, block: ContentBlock, is_first: bool = False) -> str:
        """æ ¼å¼åŒ–å•ä¸ªå†…å®¹å—"""
        if block.type == BlockType.HEADING:
            return self._format_heading(block, is_title=is_first)
        elif block.type == BlockType.PARAGRAPH:
            return self._format_paragraph(block)
        elif block.type == BlockType.LIST:
            return self._format_list(block)
        elif block.type == BlockType.QUOTE:
            return self._format_quote(block)
        elif block.type == BlockType.CODE:
            return self._format_code(block)
        elif block.type == BlockType.HORIZONTAL_RULE:
            return "â€”"  # ç®€æ´åˆ†éš”ç¬¦
        elif block.type == BlockType.IMAGE:
            # å›¾ç‰‡åœ¨å°çº¢ä¹¦æ˜¯å•ç‹¬ä¸Šä¼ çš„ï¼Œè¿™é‡Œåªè¿”å›å ä½è¯´æ˜
            if block.image_ref:
                return f"[å›¾ç‰‡: {block.image_ref.alt or 'é…å›¾'}]"
            return "[å›¾ç‰‡]"
        else:
            return block.content

    def format_page(self, page: PageContent, use_llm: bool = False) -> FormattedPage:
        """
        æ ¼å¼åŒ–å•é¡µå†…å®¹

        Args:
            page: PageContent å¯¹è±¡
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM ä¼˜åŒ–æ ¼å¼

        Returns:
            FormattedPage å¯¹è±¡
        """
        formatted_parts: list[str] = []
        emoji_count = 0

        # æ”¶é›†è¯¥é¡µå…³è”çš„å›¾ç‰‡ URLï¼ˆæŒ‰ markdown ä¸­å›¾ç‰‡å—é¡ºåºï¼‰
        image_urls: list[str] = []
        image_slots: list[int] = []

        for i, block in enumerate(page.blocks):
            # å›¾ç‰‡å—ï¼šæ”¶é›† URL ä½†ä¸è¾“å‡ºå ä½æ–‡å­—
            if block.type == BlockType.IMAGE:
                if block.image_ref:
                    image_urls.append(str(block.image_ref.path))
                    image_slots.append(len(formatted_parts))
                continue

            formatted = self._format_block(block, is_first=(i == 0))
            formatted_parts.append(formatted)

            # ç»Ÿè®¡ emoji æ•°é‡
            emoji_count += len(re.findall(r'[\U0001F300-\U0001F9FF]', formatted))

        # ä½¿ç”¨ç›²æ–‡ç©ºæ ¼è¿æ¥å„éƒ¨åˆ†
        content = self.BLOCK_SEPARATOR.join(formatted_parts)

        # å¦‚æœå¯ç”¨ LLM ä¸”å¯ç”¨ï¼Œè¿›è¡Œä¼˜åŒ–
        if use_llm and self.llm_client:
            content = self._llm_optimize(content, page)

        content = self._normalizer.normalize_rich_text(content, self.BLOCK_SEPARATOR)

        # è‹¥é¡µé¢å†…æ²¡æœ‰æ˜¾å¼å›¾ç‰‡å—ï¼Œå›é€€åˆ° splitter ç»™çš„å›¾ç‰‡åˆ—è¡¨
        if not image_urls:
            for img in page.images:
                image_urls.append(str(img.path))
                image_slots.append(len(formatted_parts))

        image_slots = self._remap_image_slots(
            image_slots=image_slots,
            original_block_count=len(formatted_parts),
            optimized_content=content,
        )

        return FormattedPage(
            page_number=page.page_number,
            content=content,
            char_count=len(content),
            emoji_count=emoji_count,
            has_proper_spacing=BRAILLE_BLANK in content,
            image_urls=image_urls,
            image_slots=image_slots,
        )

    def _remap_image_slots(
        self,
        image_slots: list[int],
        original_block_count: int,
        optimized_content: str,
    ) -> list[int]:
        """Remap image anchor slots after LLM text rewrite."""
        if not image_slots:
            return []

        old_count = max(1, original_block_count)
        new_count = max(1, len((optimized_content or "").split(self.BLOCK_SEPARATOR)))

        remapped: list[int] = []
        for slot in image_slots:
            ratio = slot / old_count
            mapped = int(round(ratio * new_count))
            mapped = max(0, min(new_count, mapped))
            remapped.append(mapped)

        # Keep stable non-decreasing order.
        for i in range(1, len(remapped)):
            if remapped[i] < remapped[i - 1]:
                remapped[i] = remapped[i - 1]

        return remapped

    @staticmethod
    def _extract_image_tokens(text: str) -> list[int]:
        """Extract ordered image token ids from content text."""
        if not text:
            return []
        token_ids: list[int] = []
        for match in RedNoteFormatter.IMAGE_TOKEN_RE.finditer(text):
            try:
                token_ids.append(int(match.group(1)))
            except (TypeError, ValueError):
                continue
        return token_ids

    def _prepare_text_for_token_parse(self, text: str) -> str:
        """Normalize standalone token lines into block separators for robust parsing."""
        if not text:
            return ""

        prepared = re.sub(
            r"(?m)^[ \t]*(<IMG_\d+>)[ \t]*$",
            lambda m: f"{self.BLOCK_SEPARATOR}{m.group(1)}{self.BLOCK_SEPARATOR}",
            text,
        )
        prepared = re.sub(
            rf"{re.escape(self.BLOCK_SEPARATOR)}{{2,}}",
            self.BLOCK_SEPARATOR,
            prepared,
        )
        return prepared

    def _inject_image_tokens(self, content: str, image_slots: list[int], image_count: int) -> str:
        """Inject <IMG_n> anchors into content based on current slots."""
        if image_count <= 0:
            return content or ""

        blocks = [block.strip() for block in (content or "").split(self.BLOCK_SEPARATOR)]
        blocks = [block for block in blocks if block]
        block_count = len(blocks)

        slots = list(image_slots or [])
        if len(slots) < image_count:
            slots.extend([block_count] * (image_count - len(slots)))
        if len(slots) > image_count:
            slots = slots[:image_count]

        normalized_slots: list[int] = []
        prev = 0
        for raw_slot in slots:
            try:
                slot = int(raw_slot)
            except (TypeError, ValueError):
                slot = block_count
            slot = max(0, min(block_count, slot))
            if slot < prev:
                slot = prev
            normalized_slots.append(slot)
            prev = slot

        output_blocks: list[str] = []
        image_idx = 0
        for block_idx in range(block_count + 1):
            while image_idx < image_count and normalized_slots[image_idx] == block_idx:
                output_blocks.append(f"<IMG_{image_idx + 1}>")
                image_idx += 1

            if block_idx < block_count:
                output_blocks.append(blocks[block_idx])

        while image_idx < image_count:
            output_blocks.append(f"<IMG_{image_idx + 1}>")
            image_idx += 1

        return self.BLOCK_SEPARATOR.join(output_blocks)

    def _strip_image_tokens_and_build_slots(
        self,
        content: str,
        image_count: int,
    ) -> tuple[str, list[int]]:
        """Parse <IMG_n> anchors from content and convert them into image slots."""
        if image_count <= 0:
            normalized = self._normalizer.normalize_rich_text(content or "", self.BLOCK_SEPARATOR)
            return normalized, []

        token_ready_content = self._prepare_text_for_token_parse(content or "")
        blocks = [block.strip() for block in token_ready_content.split(self.BLOCK_SEPARATOR)]
        blocks = [block for block in blocks if block]

        token_positions: dict[int, int] = {}
        text_blocks: list[str] = []

        for block in blocks:
            cursor = 0
            has_token = False
            for match in self.IMAGE_TOKEN_RE.finditer(block):
                has_token = True
                prefix = block[cursor:match.start()].strip()
                if prefix:
                    text_blocks.append(prefix)

                try:
                    token_id = int(match.group(1))
                except (TypeError, ValueError):
                    token_id = -1
                if 1 <= token_id <= image_count and token_id not in token_positions:
                    token_positions[token_id] = len(text_blocks)

                cursor = match.end()

            if has_token:
                suffix = block[cursor:].strip()
                if suffix:
                    text_blocks.append(suffix)
                continue

            text_blocks.append(block)

        fallback_slot = len(text_blocks)
        slots: list[int] = []
        prev = 0
        for token_id in range(1, image_count + 1):
            slot = token_positions.get(token_id, fallback_slot)
            slot = max(0, min(fallback_slot, slot))
            if slot < prev:
                slot = prev
            slots.append(slot)
            prev = slot

        text_only_content = self.BLOCK_SEPARATOR.join(text_blocks)
        normalized_content = self._normalizer.normalize_rich_text(text_only_content, self.BLOCK_SEPARATOR)
        if not normalized_content:
            normalized_content = self._normalizer.normalize_rich_text(content or "", self.BLOCK_SEPARATOR)

        return normalized_content, slots

    def _llm_optimize(self, content: str, page: PageContent) -> str:
        """ä½¿ç”¨ LLM ä¼˜åŒ–æ’ç‰ˆ"""
        try:
            # è·å–å›¾ç‰‡æƒ…æ„Ÿä¿¡æ¯
            moods = [img.mood for img in page.images] if page.images else ['neutral']
            dominant_mood = max(set(moods), key=moods.count)

            base_prompt = self._tone_system_prompt or self.FORMAT_SYSTEM_PROMPT
            system_prompt = f"""{base_prompt}

é¢å¤–ä¸Šä¸‹æ–‡ï¼ˆä»…ä¾›ä½ å‚è€ƒï¼Œä¸è¦åœ¨è¾“å‡ºä¸­å‡ºç°è¿™äº›å…ƒæ•°æ®ï¼‰ï¼š
- å›¾ç‰‡æƒ…æ„Ÿï¼š{dominant_mood}
- æ˜¯å¦ä¸ºå°é¢é¡µï¼š{page.is_cover}

ä¸¥æ ¼è¦æ±‚ï¼šè¾“å‡ºçš„ JSON ä¸­åªåŒ…å«æ’ç‰ˆåçš„æ­£æ–‡å†…å®¹ï¼Œç¦æ­¢å‡ºç°ä»»ä½•å…ƒæ•°æ®ã€æŒ‡ä»¤æˆ–è¯´æ˜æ–‡å­—ã€‚"""

            result = self.llm_client.chat_text(
                system_prompt=system_prompt,
                user_prompt=f"""è¯·ä¼˜åŒ–ä»¥ä¸‹å°çº¢ä¹¦å†…å®¹çš„æ’ç‰ˆï¼Œç©ºè¡Œå¿…é¡»ä½¿ç”¨å­—ç¬¦ â € (U+2800)ã€‚
è¿”å› JSON æ ¼å¼ã€‚

åŸå§‹å†…å®¹ï¼š
{content}""",
                temperature=0.5,
                max_tokens=2000,
                json_mode=True,
            )

            data = self.llm_client.parse_json(result.content, default={})
            if not isinstance(data, dict):
                data = {}

            # é‡å»ºå†…å®¹
            optimized_parts = []
            if 'title' in data:
                optimized_parts.append(str(data['title']))

            for section in data.get('sections', []):
                if isinstance(section, dict):
                    optimized_parts.append(str(section.get('content', '')))

            if 'ending' in data and data['ending']:
                optimized_parts.append(str(data['ending']))

            if optimized_parts:
                return PARAGRAPH_SEPARATOR.join(optimized_parts)

        except Exception as e:
            logger.warning(f"LLM optimization failed: {e}")

        return content

    def _normalize_formatted_pages(self, pages: list[FormattedPage]) -> list[FormattedPage]:
        """Apply only minimal normalization; avoid local rule-based rewriting."""
        rebuilt: list[FormattedPage] = []
        for page in pages:
            old_content = page.content
            new_content = self._normalizer.normalize_rich_text(old_content or "", self.BLOCK_SEPARATOR)
            if not new_content:
                new_content = old_content

            remapped_slots = list(page.image_slots)

            if len(remapped_slots) != len(page.image_urls):
                old_block_count = max(1, len((old_content or "").split(self.BLOCK_SEPARATOR)))
                remapped_slots = self._remap_image_slots(
                    image_slots=list(page.image_slots),
                    original_block_count=old_block_count,
                    optimized_content=new_content,
                )

            rebuilt.append(
                FormattedPage(
                    page_number=page.page_number,
                    content=new_content,
                    char_count=len(new_content),
                    emoji_count=len(re.findall(r'[\U0001F300-\U0001F9FF]', new_content)),
                    has_proper_spacing=BRAILLE_BLANK in new_content,
                    image_urls=list(page.image_urls),
                    image_slots=remapped_slots,
                )
            )
        return rebuilt

    def optimize_document_pages(
        self,
        pages: list[FormattedPage],
        use_llm: bool = True,
    ) -> list[FormattedPage]:
        """Globally rewrite all pages once to improve cross-page continuity."""
        if not pages:
            return pages
        if not use_llm or not self.llm_client:
            return self._normalize_formatted_pages(pages)

        page_payload = [
            {
                "page_number": page.page_number,
                "content": self._inject_image_tokens(
                    page.content,
                    page.image_slots,
                    len(page.image_urls),
                ),
                "has_images": bool(page.image_urls),
                "image_count": len(page.image_urls),
                "char_count": len(page.content or ""),
            }
            for page in pages
        ]
        payload_json = json.dumps(
            {
                "total_pages": len(pages),
                "pages": page_payload,
                "output_contract": {
                    "must_keep_page_count": True,
                    "style": "single_continuous_story_across_pages",
                    "forbid_restart_after_page_1": True,
                },
            },
            ensure_ascii=False,
            indent=2,
        )

        try:
            result = self.llm_client.chat_text(
                system_prompt=self.DOCUMENT_CONTINUITY_SYSTEM_PROMPT,
                user_prompt=(
                    "è¯·åŸºäºä¸‹é¢çš„ JSON è¾“å…¥é‡å†™æ•´ç¯‡åˆ†é¡µè‰ç¨¿ã€‚\n"
                    "å…³é”®ï¼šç¬¬2é¡µå¼€å§‹å¿…é¡»æ‰¿æ¥ï¼Œä¸è¦æ¯é¡µé‡æ–°èµ·æ ‡é¢˜ã€‚\n"
                    "è¯·ç›´æ¥è¾“å‡º JSONï¼ˆåªå« pages å­—æ®µï¼‰ã€‚\n\n"
                    f"{payload_json}"
                ),
                temperature=0.3,
                max_tokens=4000,
                json_mode=True,
            )

            data = self.llm_client.parse_json(result.content, default={})
            if not isinstance(data, dict):
                return pages

            raw_pages = data.get("pages")
            if not isinstance(raw_pages, list):
                return pages

            if len(raw_pages) != len(pages):
                logger.warning(
                    "Document continuity rewrite returned mismatched page count: "
                    f"expected={len(pages)}, got={len(raw_pages)}"
                )
                return pages

            rebuilt_pages: list[FormattedPage] = []
            for idx, old_page in enumerate(pages):
                raw_item = raw_pages[idx]
                content_candidate = ""
                if isinstance(raw_item, dict):
                    content_candidate = str(raw_item.get("content", "")).strip()
                elif isinstance(raw_item, str):
                    content_candidate = raw_item.strip()

                if not content_candidate:
                    content_candidate = old_page.content

                tokenized_fallback = self._inject_image_tokens(
                    old_page.content,
                    old_page.image_slots,
                    len(old_page.image_urls),
                )

                if len(old_page.image_urls) > 0:
                    expected_tokens = [idx + 1 for idx in range(len(old_page.image_urls))]
                    candidate_tokens = self._extract_image_tokens(content_candidate)
                    if candidate_tokens != expected_tokens:
                        logger.warning(
                            "Page %s image tokens invalid, fallback to tokenized source. expected=%s got=%s",
                            old_page.page_number,
                            expected_tokens,
                            candidate_tokens,
                        )
                        content_candidate = tokenized_fallback

                new_content, semantic_slots = self._strip_image_tokens_and_build_slots(
                    content_candidate,
                    image_count=len(old_page.image_urls),
                )
                if not new_content:
                    new_content = old_page.content

                if len(semantic_slots) == len(old_page.image_urls):
                    remapped_slots = semantic_slots
                else:
                    old_block_count = max(1, len((old_page.content or "").split(self.BLOCK_SEPARATOR)))
                    remapped_slots = self._remap_image_slots(
                        image_slots=list(old_page.image_slots),
                        original_block_count=old_block_count,
                        optimized_content=new_content,
                    )

                rebuilt_pages.append(
                    FormattedPage(
                        page_number=old_page.page_number,
                        content=new_content,
                        char_count=len(new_content),
                        emoji_count=len(re.findall(r'[\U0001F300-\U0001F9FF]', new_content)),
                        has_proper_spacing=BRAILLE_BLANK in new_content,
                        image_urls=list(old_page.image_urls),
                        image_slots=remapped_slots,
                    )
                )

            return self._normalize_formatted_pages(rebuilt_pages)

        except Exception as exc:
            logger.warning(f"Document continuity rewrite failed: {exc}")
            return self._normalize_formatted_pages(pages)

    def format_all_pages(
        self,
        pages: list[PageContent],
        use_llm: bool = False
    ) -> list[FormattedPage]:
        """
        æ ¼å¼åŒ–æ‰€æœ‰é¡µé¢

        Args:
            pages: PageContent åˆ—è¡¨
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM ä¼˜åŒ–

        Returns:
            FormattedPage åˆ—è¡¨
        """
        formatted_pages = []
        for page in pages:
            formatted = self.format_page(page, use_llm=use_llm)
            formatted_pages.append(formatted)
            logger.info(
                f"Page {page.page_number}: {formatted.char_count} chars, "
                f"{formatted.emoji_count} emojis, proper_spacing={formatted.has_proper_spacing}"
            )
        return formatted_pages

    def add_ending(self, formatted_page: FormattedPage, style: str = 'default') -> FormattedPage:
        """
        ä¸ºé¡µé¢æ·»åŠ ç»“å°¾ï¼ˆä¸€èˆ¬ä¸éœ€è¦è°ƒç”¨ï¼‰

        Args:
            formatted_page: æ ¼å¼åŒ–åçš„é¡µé¢
            style: ç»“å°¾é£æ ¼

        Returns:
            æ›´æ–°åçš„ FormattedPage
        """
        # å¤§éƒ¨åˆ†æƒ…å†µä¸åŠ ç»“å°¾ï¼Œå†…å®¹è‡ªç„¶æ”¶å°¾å°±å¥½
        endings = {
            'default': "",  # ä¸åŠ ç»“å°¾
            'simple': f"\n{BRAILLE_BLANK}\n.",
            'question': f"\n{BRAILLE_BLANK}\nä½ ä»¬è§‰å¾—å‘¢",
        }

        ending = endings.get(style, "")
        if not ending:
            return formatted_page

        new_content = formatted_page.content + ending

        return FormattedPage(
            page_number=formatted_page.page_number,
            content=new_content,
            char_count=len(new_content),
            emoji_count=formatted_page.emoji_count,
            has_proper_spacing=formatted_page.has_proper_spacing,
            image_urls=formatted_page.image_urls,
            image_slots=formatted_page.image_slots,
        )
